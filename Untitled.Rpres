Word Prediction Application
========================================================
author: Ben Karmay
date: October 2018
font-family: arial
width: 1600
height: 900

This presentation will provide a brief background on the application, how to use it and how it works.

View the working App: <https://bkarmay.shinyapps.io/bkarmayshinyapp/>

View Sourse code for the App:

About the App
========================================================

The goal of this project is to produce a Shiny App product that uses prediction algorithm to generate a next possible word based on text input buy the user.


The predictive model is based on the concept of n-gram sequence of words used in Natural Language Processing.



Word Predictions with the App
========================================================

The application has an input text field where you can type in. Type a word or incomplete phrase in the input text field and click on the Predict button. 

 
Your input phrase along with suggested next words will be shown to the right of the textbox with a few seconds delay as you type.

View and try the working App: <https://bkarmay.shinyapps.io/bkarmayshinyapp/>


How It Works
========================================================

### Generating Next Word Suggestions

The Word Prediction app uses an [N-gram language model] (https://en.wikipedia.org/wiki/Language_model#n-gram_models) created from samples of twitter, blog and news text taken from a corpus. 

The corpus text can be downloaded [here] (https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).


How the model Works
========================================================

### Creating the Model

<small>
The 4-gram, Trigram, Bigram and Unigram models were created using the `dfm` function from [quanteda] (https://cran.r-project.org/web/packages/quanteda/vignettes/quickstart.html) package which provides capability for cleaning up text and extracting the unique n-grams along with their frequencies in the training corpus.

Once data has been cleaned up and we've extracted the count of occurence for unique n-grams found in the text, we compute the maximum likelihood estimate (MLE) for each n-gram using the ff. formula:

<center>![MLE Formula](mle.jpg)</center>

For Unigrams, Kneser-Ney Probability was also computed. As described in this [article] (http://smithamilli.com/blog/kneser-ney/), the unigram Kneser-Ney probability is the number of unique words the unigram follows divided by all bigrams.

***

Summary of Clean-up steps:

1. Changing text to lowercase
2. Removing non-ASCII characters
3. Removing punctuation and symbols (i.e. #, etc.)
4. Removing numbers
5. Removing sentences with words in the profanity list downloaded [here] (http://www.bannedwordlist.com/)

Additional Notes:
* Stemming and removal of stop words from the text was not done to retain the proper context when creating the n-grams.
* Addition of start and end of sentence markers was also skipped as the intent for the application is to be able to provide predictions even for shorter input phrases.
* The full code for model creation may be found in the [github repo] (https://github.com/Ceathiel/DataScienceCapstone/)
</small>